{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN, LSTM, and GRU\n",
    "\n",
    "## Introduction\n",
    "This notebook contains some of the details for the implementation of Recurrent Neural Networks (RNNs) and two variants of it, the LSTM and GRU. To see the full implementation, see the source code [here](https://github.com/hongjos/rnn). Recurrent Neural Networks (RNNs) were introduced in the 1980s, with the aim of modeling sequences of data, such as text, music, and time series. However, early RNN models faced challenges with learning long-term dependencies, which limited their effectiveness. To deal with this, variants of the RNN were introduced later such as the LSTM and GRU.\n",
    "\n",
    "To test and compare implementations of the RNN, LSTM and GRU, we will use the [IMDB Movie Reviews dataset](https://keras.io/api/datasets/imdb/) for sentiment classification.\n",
    "\n",
    "This notebook just gives a standard implementation of the models to get a better understanding of them. You shouldn't actually use this implementation to do real life stuff. You should be using the ones from [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) or [TensorFlow](https://www.tensorflow.org/guide/keras/rnn).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelims\n",
    "The implementation for the RNN will primariy use `numpy`. Other libraries used in this notebook are just for getting the data and visualizing the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn import RNN\n",
    "from lstm import LSTM\n",
    "from gru import GRU\n",
    "\n",
    "import numpy as np\n",
    "import math, time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.datasets import imdb # used for evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "To represent words as numerical values, we can represent each word as a vector with a 1 in the index that identifies the word and 0s elsewhere. For example, if you were a baby and only knew the words 'goo' and 'ga'. We can use 0 to represent the 'goo' and 1 for 'ga'. Then the phrase 'goo goo ga ga' would look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {'goo' : 0, 'ga' : 1}\n",
    "phrase = ['goo', 'goo', 'ga', 'ga']\n",
    "\n",
    "[vocab[x] for x in phrase]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-hot encoding would look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = np.zeros((len(phrase), 2))\n",
    "\n",
    "for i, word in enumerate(phrase):\n",
    "    seq[i][vocab[word]] = 1\n",
    "\n",
    "seq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test if the implementations are working as intended, we will fit the model to a very small dataset with 3 movie reviews for sentiment classification.\n",
    "\n",
    "| Review | Class |\n",
    "| -------- | ---- |\n",
    "| this movie is good | Positive |\n",
    "| this movie is bad | Negative |\n",
    "| this movie is not good | Negative |\n",
    "\n",
    "Our test set will contain the review:\n",
    "| Review | Class |\n",
    "| -------- | ---- |\n",
    "| this movie is not bad | Positive |\n",
    "\n",
    "0 will represent the positive sentiment class, and 1 for the negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]],\n",
       "\n",
       "       [[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]],\n",
       "\n",
       "       [[0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]],\n",
       "\n",
       "       [[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {\"this\" : 0, \"movie\" : 1, \"is\" : 2, \"good\": 3, \"bad\" : 4, \"not\" : 5}\n",
    "vocab_size = 6\n",
    "\n",
    "xtrain_string = [\"this movie is good\", \"this movie is bad\", \"this movie is not good\"]\n",
    "ytrain_val = [0, 1, 1]\n",
    "xtest_string = [\"this movie is not bad\"]\n",
    "ytest_val = [0]\n",
    "\n",
    "xtrain, ytrain, xtest, ytest = [], [], [], []\n",
    "\n",
    "def one_hot_x(lst, size=6):\n",
    "    \"\"\"\n",
    "    One-hot enconding for a list of strings sequences.\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "\n",
    "    # go thru list and make string sequence into vectors\n",
    "    for s in lst:\n",
    "        seq = np.zeros((len(s.split()), size, 1))\n",
    "\n",
    "        # one-hot encode the sequence\n",
    "        for i,val in enumerate(s.split()):\n",
    "            seq[i][vocab[val]] = 1\n",
    "        ret.append(seq) # add to list\n",
    "        \n",
    "    return ret\n",
    "\n",
    "def one_hot_y(lst, num_class=2):\n",
    "    \"\"\"\n",
    "    One-hot enconding for the classes.\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    # go thru list and make classes into vectors.\n",
    "    for x in lst:\n",
    "        vec = np.zeros((num_class, 1))\n",
    "        vec[x] = 1 # one-hot encode the class\n",
    "        ret.append(vec) # add to list\n",
    "    return ret\n",
    "\n",
    "\n",
    "xtrain, xtest = one_hot_x(xtrain_string), one_hot_x(xtest_string)\n",
    "ytrain, ytest = one_hot_y(ytrain_val), one_hot_y(ytest_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png?9ea4417fc145b9346a3e288801dbdfdc\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da\n",
      "db\n"
     ]
    }
   ],
   "source": [
    "test = {'a' : 1, 'b' : 2}\n",
    "for t in test:\n",
    "    name = 'd' + t\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn\n",
      "Epoch 0, training loss: 1.8445101635539731\n",
      "Epoch 5, training loss: 1.3735530882162283\n",
      "Epoch 10, training loss: 0.580944937868262\n",
      "Epoch 15, training loss: 0.22389386677896467\n",
      "Epoch 20, training loss: 0.11914532106805094\n",
      "Epoch 25, training loss: 0.07427093843491467\n",
      "lstm\n",
      "Epoch 0, training loss: 1.9206830636450158\n",
      "Epoch 5, training loss: 1.8123636807488614\n",
      "Epoch 10, training loss: 0.12019302831117626\n",
      "Epoch 15, training loss: 0.03901948314936534\n",
      "Epoch 20, training loss: 0.02313178273642524\n",
      "Epoch 25, training loss: 0.016333873033484207\n",
      "gru\n",
      "Epoch 0, training loss: 3.3360171694868272\n",
      "Epoch 5, training loss: 1.093418181528314\n",
      "Epoch 10, training loss: 0.07985457717012517\n",
      "Epoch 15, training loss: 0.03903190652096257\n",
      "Epoch 20, training loss: 0.02518610745585075\n",
      "Epoch 25, training loss: 0.0183439745802757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rnn import RNN\n",
    "from lstm import LSTM\n",
    "from gru import GRU\n",
    "import numpy as np\n",
    "\n",
    "vocab_size = 5\n",
    "\n",
    "###\n",
    "seq1 = np.zeros((3,vocab_size,1))\n",
    "s1 = [0,2,4]\n",
    "\n",
    "for i,val in enumerate(s1):\n",
    "    seq1[i][val] = 1\n",
    "\n",
    "y1 = np.zeros((2,1))\n",
    "y1[0] = 1\n",
    "\n",
    "\n",
    "###\n",
    "seq2 = np.zeros((4,vocab_size,1))\n",
    "s2 = [1,3,2,0]\n",
    "\n",
    "for i,val in enumerate(s2):\n",
    "    seq2[i][val] = 1\n",
    "\n",
    "y2 = np.zeros((2,1))\n",
    "y2[1] = 1\n",
    "\n",
    "###\n",
    "seq3 = np.zeros((3,vocab_size,1))\n",
    "s3 = [0,3,1]\n",
    "\n",
    "for i,val in enumerate(s3):\n",
    "    seq3[i][val] = 1\n",
    "\n",
    "y3 = np.zeros((2,1))\n",
    "y3[0] = 1\n",
    "\n",
    "Xtrain = [seq1, seq2]\n",
    "Ytrain = [y1, y2]\n",
    "\n",
    "Xtest = [seq3]\n",
    "Ytest = [y3]\n",
    "\n",
    "rnntest = RNN(input_dim=vocab_size, output_dim=2, hidden_dim=5, learning_rate=.05)\n",
    "lstmtest = LSTM(input_dim=vocab_size, output_dim=2, hidden_dim=5, learning_rate=.3)\n",
    "grutest = GRU(input_dim=vocab_size, output_dim=2, hidden_dim=5, learning_rate=.3)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "print(\"rnn\")\n",
    "rnntest.fit(Xtrain, Ytrain, num_epochs=epochs)\n",
    "rnntest.evaluate(Xtest, Ytest)\n",
    "\n",
    "print(\"lstm\")\n",
    "lstmtest.fit(Xtrain, Ytrain, num_epochs=epochs)\n",
    "lstmtest.evaluate(Xtest, Ytest)\n",
    "\n",
    "print(\"gru\")\n",
    "grutest.fit(Xtrain, Ytrain, num_epochs=epochs)\n",
    "grutest.evaluate(Xtest, Ytest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on IMDB Dataset\n",
    "\n",
    "Words are indexed by overall frequency in the dataset e.g. `3` encodes the third most frequent word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "vocab_size = 100\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "index_to_word = dict([(value,key) for (key,value) in word_index.items()])\n",
    "\n",
    "(X_train_small, y_train_small) = (X_train[0:100], y_train[0:100])\n",
    "(X_test_small, y_test_small) = (X_test[0:100], y_test[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(input):\n",
    "    sentence = []\n",
    "    for i in input:\n",
    "        sentence.append(index_to_word.get(i-3, '!'))\n",
    "    print(\" \".join(sentence))\n",
    "    return sentence\n",
    "\n",
    "def one_hot(value, vec_size):\n",
    "    \"\"\"\n",
    "    Given a scalar returns the one-hot encoding vector.\n",
    "    \"\"\"\n",
    "    # initialize input matrix\n",
    "    x = np.zeros((vec_size, 1))\n",
    "    x[value] = 1\n",
    "        \n",
    "    return x\n",
    "\n",
    "def subtract_one(X):\n",
    "    \"\"\"\n",
    "    Subtracts one from each value.\n",
    "    \"\"\"\n",
    "    for i in range(X.size):\n",
    "        X[i] = [x-1 for x in X[i]]\n",
    "    \n",
    "    return X\n",
    "\n",
    "def one_hot_x(X):\n",
    "    oh = [None] * X.size\n",
    "    for i, input in enumerate(X):\n",
    "        xx = np.zeros((len(input), vocab_size, 1))\n",
    "        for j, val in enumerate(input):\n",
    "            xx[j][val] = 1\n",
    "        oh[i] = xx\n",
    "    \n",
    "    return oh\n",
    "\n",
    "def one_hot_y(Y):\n",
    "    oh = [None] * Y.size\n",
    "    for i, y in enumerate(Y):\n",
    "        oh[i] = one_hot(y, 2)\n",
    "    \n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract ones\n",
    "X_train_small = subtract_one(X_train_small)\n",
    "X_test_small = subtract_one(X_test_small)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = one_hot_x(X_train_small)\n",
    "ty = one_hot_y(y_train_small)\n",
    "testingx = one_hot_x(X_test_small)\n",
    "testingy = one_hot_y(y_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training loss: 38.02021500786034\n",
      "Epoch 2, training loss: 37.88570052221522\n",
      "Epoch 3, training loss: 37.76739698482241\n",
      "Epoch 4, training loss: 37.66479923610425\n",
      "Epoch 5, training loss: 37.57031353036847\n",
      "Epoch 6, training loss: 37.48792986875993\n",
      "Epoch 7, training loss: 37.406753961560895\n",
      "Epoch 8, training loss: 37.32507970195589\n",
      "Epoch 9, training loss: 37.24167319569322\n",
      "Epoch 10, training loss: 37.158977623183155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[38.02021500786034,\n",
       " 37.88570052221522,\n",
       " 37.76739698482241,\n",
       " 37.66479923610425,\n",
       " 37.57031353036847,\n",
       " 37.48792986875993,\n",
       " 37.406753961560895,\n",
       " 37.32507970195589,\n",
       " 37.24167319569322,\n",
       " 37.158977623183155]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rnn import RNN\n",
    "RNNModel = RNN(input_dim=vocab_size, hidden_dim=10, output_dim=2, learning_rate=1e-4)\n",
    "RNNModel.fit(tx, ty, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNNModel.evaluate(testingx, testingy)\n",
    "# RNNModel.evaluate(tx, ty)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
