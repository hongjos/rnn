{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from rnn import RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png?9ea4417fc145b9346a3e288801dbdfdc\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da\n",
      "db\n"
     ]
    }
   ],
   "source": [
    "test = {'a' : 1, 'b' : 2}\n",
    "for t in test:\n",
    "    name = 'd' + t\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn\n",
      "Epoch 1, training loss: 2.734034495914997\n",
      "Epoch 6, training loss: 0.33630521594798235\n",
      "Epoch 11, training loss: 0.08978918717463699\n",
      "Epoch 16, training loss: 0.048660762961987264\n",
      "Epoch 21, training loss: 0.03240331323110031\n",
      "Epoch 26, training loss: 0.02370043125654055\n",
      "lstm\n",
      "Epoch 1, training loss: 1.602626223283067\n",
      "Epoch 6, training loss: 1.5555612756631487\n",
      "Epoch 11, training loss: 1.4957488362716647\n",
      "Epoch 16, training loss: 1.3573726614268251\n",
      "Epoch 21, training loss: 0.6708676598470689\n",
      "Epoch 26, training loss: 0.108431770499992\n",
      "gru\n",
      "Epoch 1, training loss: 1.747618307270614\n",
      "Epoch 6, training loss: 1.180046407174885\n",
      "Epoch 11, training loss: 0.4181313641136248\n",
      "Epoch 16, training loss: 0.1370598308024285\n",
      "Epoch 21, training loss: 0.07034685977617056\n",
      "Epoch 26, training loss: 0.04451749635677488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rnn import RNN\n",
    "from lstm import LSTM\n",
    "from gru import GRU\n",
    "import numpy as np\n",
    "\n",
    "vocab_size = 5\n",
    "\n",
    "###\n",
    "seq1 = np.zeros((3,vocab_size,1))\n",
    "s1 = [0,2,4]\n",
    "\n",
    "for i,val in enumerate(s1):\n",
    "    seq1[i][val] = 1\n",
    "\n",
    "y1 = np.zeros((2,1))\n",
    "y1[0] = 1\n",
    "\n",
    "\n",
    "###\n",
    "seq2 = np.zeros((4,vocab_size,1))\n",
    "s2 = [1,3,2,0]\n",
    "\n",
    "for i,val in enumerate(s2):\n",
    "    seq2[i][val] = 1\n",
    "\n",
    "y2 = np.zeros((2,1))\n",
    "y2[1] = 1\n",
    "\n",
    "###\n",
    "seq3 = np.zeros((3,vocab_size,1))\n",
    "s3 = [0,3,1]\n",
    "\n",
    "for i,val in enumerate(s3):\n",
    "    seq3[i][val] = 1\n",
    "\n",
    "y3 = np.zeros((2,1))\n",
    "y3[0] = 1\n",
    "\n",
    "Xtrain = [seq1, seq2]\n",
    "Ytrain = [y1, y2]\n",
    "\n",
    "Xtest = [seq3]\n",
    "Ytest = [y3]\n",
    "\n",
    "# rnntest = RNN(input_dim=vocab_size, output_dim=2, hidden_dim=5, learning_rate=.1)\n",
    "lstmtest = LSTM(input_dim=vocab_size, output_dim=2, hidden_dim=5, learning_rate=.2)\n",
    "# grutest = GRU(input_dim=vocab_size, output_dim=2, hidden_dim=5, learning_rate=.2)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "print(\"rnn\")\n",
    "# rnntest.fit(Xtrain, Ytrain, num_epochs=epochs)\n",
    "# rnntest.evaluate(Xtest, Ytest)\n",
    "\n",
    "print(\"lstm\")\n",
    "lstmtest.fit(Xtrain, Ytrain, num_epochs=epochs)\n",
    "lstmtest.evaluate(Xtest, Ytest)\n",
    "\n",
    "print(\"gru\")\n",
    "# grutest.fit(Xtrain, Ytrain, num_epochs=epochs)\n",
    "# grutest.evaluate(Xtest, Ytest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on IMDB Dataset\n",
    "\n",
    "Words are indexed by overall frequency in the dataset e.g. `3` encodes the third most frequent word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "vocab_size = 100\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "index_to_word = dict([(value,key) for (key,value) in word_index.items()])\n",
    "\n",
    "(X_train_small, y_train_small) = (X_train[0:100], y_train[0:100])\n",
    "(X_test_small, y_test_small) = (X_test[0:100], y_test[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(input):\n",
    "    sentence = []\n",
    "    for i in input:\n",
    "        sentence.append(index_to_word.get(i-3, '!'))\n",
    "    print(\" \".join(sentence))\n",
    "    return sentence\n",
    "\n",
    "def one_hot(value, vec_size):\n",
    "    \"\"\"\n",
    "    Given a scalar returns the one-hot encoding vector.\n",
    "    \"\"\"\n",
    "    # initialize input matrix\n",
    "    x = np.zeros((vec_size, 1))\n",
    "    x[value] = 1\n",
    "        \n",
    "    return x\n",
    "\n",
    "def subtract_one(X):\n",
    "    \"\"\"\n",
    "    Subtracts one from each value.\n",
    "    \"\"\"\n",
    "    for i in range(X.size):\n",
    "        X[i] = [x-1 for x in X[i]]\n",
    "    \n",
    "    return X\n",
    "\n",
    "def one_hot_x(X):\n",
    "    oh = [None] * X.size\n",
    "    for i, input in enumerate(X):\n",
    "        xx = np.zeros((len(input), vocab_size, 1))\n",
    "        for j, val in enumerate(input):\n",
    "            xx[j][val] = 1\n",
    "        oh[i] = xx\n",
    "    \n",
    "    return oh\n",
    "\n",
    "def one_hot_y(Y):\n",
    "    oh = [None] * Y.size\n",
    "    for i, y in enumerate(Y):\n",
    "        oh[i] = one_hot(y, 2)\n",
    "    \n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract ones\n",
    "X_train_small = subtract_one(X_train_small)\n",
    "X_test_small = subtract_one(X_test_small)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = one_hot_x(X_train_small)\n",
    "ty = one_hot_y(y_train_small)\n",
    "testingx = one_hot_x(X_test_small)\n",
    "testingy = one_hot_y(y_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training loss: 38.02021500786034\n",
      "Epoch 2, training loss: 37.88570052221522\n",
      "Epoch 3, training loss: 37.76739698482241\n",
      "Epoch 4, training loss: 37.66479923610425\n",
      "Epoch 5, training loss: 37.57031353036847\n",
      "Epoch 6, training loss: 37.48792986875993\n",
      "Epoch 7, training loss: 37.406753961560895\n",
      "Epoch 8, training loss: 37.32507970195589\n",
      "Epoch 9, training loss: 37.24167319569322\n",
      "Epoch 10, training loss: 37.158977623183155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[38.02021500786034,\n",
       " 37.88570052221522,\n",
       " 37.76739698482241,\n",
       " 37.66479923610425,\n",
       " 37.57031353036847,\n",
       " 37.48792986875993,\n",
       " 37.406753961560895,\n",
       " 37.32507970195589,\n",
       " 37.24167319569322,\n",
       " 37.158977623183155]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rnn import RNN\n",
    "RNNModel = RNN(input_dim=vocab_size, hidden_dim=10, output_dim=2, learning_rate=1e-4)\n",
    "RNNModel.fit(tx, ty, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNNModel.evaluate(testingx, testingy)\n",
    "# RNNModel.evaluate(tx, ty)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
